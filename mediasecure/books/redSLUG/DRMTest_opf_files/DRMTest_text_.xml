<?xml version = "1.0" encoding = "UTF-8"?>

<FlipBook ID = "6cd12cb663e67e49dc5a0f7efe433d85">
   <page pageID = "item0" urlID = "David Mackay Book1_1.swf" pageNumber = "1">
      <text>Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981 You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links. 18 1 — Introduction to Information Theory a source bit is flipped. Are parity bits or source bits more likely to be among these three flipped bits, or are all seven bits equally likely to be corrupted when the noise vector has weight two? The Hamming code is in fact completely symmetric in the protection it affords to the seven bits (assuming a binary symmetric channel). [This symmetry can be proved by showing that the role of a parity bit can be exchanged with a source bit and the resulting code is still a (7, 4) Hamming code; see below.] The probability that any one bit ends up corrupted is the same for all seven bits. So the probability of bit error (for the source bits) is simply three sevenths of the probability of block error. 3 7pB ! 9f2. (1.48) pb ! Symmetry of the Hamming (7, 4) code To prove that the (7, 4) code protects all bits equally, we start from the parity-check matrix ! " # $. (1.49) 1 1 1 0 1 0 0 0 1 1 1 0 1 0 1 0 1 1 0 0 1 H = The symmetry among the seven transmitted bits will be easiest to see if we reorder the seven bits using the permutation (t1t2t3t4t5t6t7) " (t5t2t3t4t1t6t7). Then we can rewrite H thus: ! " # $. (1.50) 1 1 1 0 1 0 0 0 1 1 1 0 1 0 0 0 1 1 1 0 1 H = Now, if we take any two parity constraints that t satisfies and add them together, we get another parity constraint. For example, row 1 asserts t5 + t2 + t3 + t1 = even, and row 2 asserts t2 + t3 + t4 + t6 = even, and the sum of these two constraints is t5 + 2t2 + 2t3 + t1 + t4 + t6 = even; (1.51) we can drop the terms 2t2 and 2t3, since they are even whatever t2 and t3 are; thus we have derived the parity constraint t5 + t1 + t4 + t6 = even, which we can if we wish add into the parity-check matrix as a fourth row. [The set of vectors satisfying Ht = 0 will not be changed.] We thus define ! # 1 1 1 0 1 0 0 0 1 1 1 0 1 0 0 0 1 1 1 0 1 1 0 0 1 1 1 0 %%" &amp;&amp;$ H! = . (1.52) The fourth row is the sum (modulo two) of the top two rows. Notice that the second, third, and fourth rows are all cyclic shifts of the top row. If, having added the fourth redundant constraint, we drop the first constraint, we obtain a new parity-check matrix H!!, ! " # $, (1.53) 0 1 1 1 0 1 0 0 0 1 1 1 0 1 1 0 0 1 1 1 0 H!! = which still satisfies H!!t = 0 for all codewords, and which looks just like the starting H in (1.50), except that all the columns have shifted along one</text>
   </page>
   <page pageID = "item1" urlID = "David Mackay Book1_2.swf" pageNumber = "2">
      <text>Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981 You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links. 1.6: Solutions 19 to the right, and the rightmost column has reappeared at the left (a cyclic permutation of the columns). This establishes the symmetry among the seven bits. Iterating the above procedure five more times, we can make a total of seven different H matrices for the same original code, each of which assigns each bit to a different role. We may also construct the super-redundant seven-row parity-check matrix for the code, # ! 1 1 1 0 1 0 0 0 1 1 1 0 1 0 0 0 1 1 1 0 1 1 0 0 1 1 1 0 0 1 0 0 1 1 1 1 0 1 0 0 1 1 1 1 0 1 0 0 1 %%%%%%%%" &amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;$ . (1.54) H!!! = This matrix is ‘redundant’ in the sense that the space spanned by its rows is only three-dimensional, not seven. This matrix is also a cyclic matrix. Every row is a cyclic permutation of the top row. Cyclic codes: if there is an ordering of the bits t1 . . . tN such that a linear code has a cyclic parity-check matrix, then the code is called a cyclic code. The codewords of such a code also have cyclic properties: any cyclic permutation of a codeword is a codeword. For example, the Hamming (7, 4) code, with its bits ordered as above, consists of all seven cyclic shifts of the codewords 1110100 and 1011000, and the codewords 0000000 and 1111111. Cyclic codes are a cornerstone of the algebraic approach to error-correcting codes. We won’t use them again in this book, however, as they have been superceded by sparse-graph codes (Part VI). Solution to exercise 1.7 (p.13). There are fifteen non-zero noise vectors which give the all-zero syndrome; these are precisely the fifteen non-zero codewords of the Hamming code. Notice that because the Hamming code is linear, the sum of any two codewords is a codeword. Graphs corresponding to codes Solution to exercise 1.9 (p.14). When answering this question, you will prob-ably find that it is easier to invent new codes than to find optimal decoders for them. There are many ways to design codes, and what follows is just one possible train of thought. We make a linear block code that is similar to the (7, 4) Hamming code, but bigger. Many codes can be conveniently expressed in terms of graphs. In fig-ure 1.13, we introduced a pictorial representation of the (7, 4) Hamming code. Figure 1.20. The graph of the (7, 4) Hamming code. The 7 circles are the bit nodes and the 3 squares are the parity-check nodes. If we replace that figure’s big circles, each of which shows that the parity of four particular bits is even, by a ‘parity-check node’ that is connected to the four bits, then we obtain the representation of the (7, 4) Hamming code by a bipartite graph as shown in figure 1.20. The 7 circles are the 7 transmitted bits. The 3 squares are the parity-check nodes (not to be confused with the 3 parity-check bits, which are the three most peripheral circles). The graph is a ‘bipartite’ graph because its nodes fall into two classes – bits and checks</text>
   </page>
   <page pageID = "item2" urlID = "David Mackay Book1_3.swf" pageNumber = "3">
      <text>Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981 You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links. 20 1 — Introduction to Information Theory – and there are edges only between nodes in different classes. The graph and the code’s parity-check matrix (1.30) are simply related to each other: each parity-check node corresponds to a row of H and each bit node corresponds to a column of H; for every 1 in H, there is an edge between the corresponding pair of nodes. Having noticed this connection between linear codes and graphs, one way to invent linear codes is simply to think of a bipartite graph. For example, a pretty bipartite graph can be obtained from a dodecahedron by calling the vertices of the dodecahedron the parity-check nodes, and putting a transmitted bit on each edge in the dodecahedron. This construction defines a parity- Figure 1.21. The graph defining the (30, 11) dodecahedron code. The circles are the 30 transmitted bits and the triangles are the 20 parity checks. One parity check is redundant. check matrix in which every column has weight 2 and every row has weight 3. [The weight of a binary vector is the number of 1s it contains.] This code has N = 30 bits, and it appears to have Mapparent = 20 parity-check constraints. Actually, there are only M = 19 independent constraints; the 20th constraint is redundant (that is, if 19 constraints are satisfied, then the 20th is automatically satisfied); so the number of source bits is K = N −M = 11. The code is a (30, 11) code. It is hard to find a decoding algorithm for this code, but we can estimate its probability of error by finding its lowest-weight codewords. If we flip all the bits surrounding one face of the original dodecahedron, then all the parity checks will be satisfied; so the code has 12 codewords of weight 5, one for each face. Since the lowest-weight codewords have weight 5, we say that the code has distance d = 5; the (7, 4) Hamming code had distance 3 and could correct all single bit-flip errors. A code with distance 5 can correct all double bit-flip errors, but there are some triple bit-flip errors that it cannot correct. So the error probability of this code, assuming a binary symmetric channel, will be dominated, at least for low noise levels f, by a term of order f 3, perhaps something like Figure 1.22. Graph of a rate-1/4 low-density parity-check code (Gallager code) with blocklength N = 16, and M = 12 parity-check constraints. Each white circle represents a transmitted bit. Each bit participates in j = 3 constraints, represented by squares. The edges between nodes were placed at random. (See Chapter 47 for more.) ' 5 3 ( f3(1 − f)27. (1.55) 12 Of course, there is no obligation to make codes whose graphs can be rep-resented on a plane, as this one can; the best linear codes, which have simple graphical descriptions, have graphs that are more tangled, as illustrated by the tiny (16, 4) code of figure 1.22. Furthermore, there is no reason for sticking to linear codes; indeed some nonlinear codes – codes whose codewords cannot be defined by a linear equa-tion like Ht = 0 – have very good properties. But the encoding and decoding of a nonlinear code are even trickier tasks. Solution to exercise 1.10 (p.14). First let’s assume we are making a linear code and decoding it with syndrome decoding. If there are N transmitted bits, then the number of possible error patterns of weight up to two is ' N 2 ( + ' N 1 ( + ' N 0 ( . (1.56) For N = 14, that’s 91 + 14 + 1 = 106 patterns. Now, every distinguishable error pattern must give rise to a distinct syndrome; and the syndrome is a list of M bits, so the maximum possible number of syndromes is 2M. For a (14, 8) code, M = 6, so there are at most 26 = 64 syndromes. The number of possible error patterns of weight up to two, 106, is bigger than the number of syndromes, 64, so we can immediately rule out the possibility that there is a (14, 8) code that is 2-error-correcting.</text>
   </page>
   <page pageID = "item3" urlID = "David Mackay Book1_4.swf" pageNumber = "4">
      <text>Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981 You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links. 1.6: Solutions 21 The same counting argument works fine for nonlinear codes too. When the decoder receives r = t + n, his aim is to deduce both t and n from r. If it is the case that the sender can select any transmission t from a code of size St, and the channel can select any noise vector from a set of size Sn, and those two selections can be recovered from the received bit string r, which is one of at most 2N possible strings, then it must be the case that StSn $ 2N. (1.57) So, for a (N,K) two-error-correcting code, whether linear or nonlinear, )' ( + ' N 1 ( + ' N 0 (* $ 2N. (1.58) N 2 2K Solution to exercise 1.11 (p.14). There are various strategies for making codes that can correct multiple errors, and I strongly recommend you think out one or two of them for yourself. If your approach uses a linear code, e.g., one with a collection of M parity checks, it is helpful to bear in mind the counting argument given in the previous exercise, in order to anticipate how many parity checks, M, you might need. Examples of codes that can correct any two errors are the (30, 11) dodeca-hedron code on page 20, and the (15, 6) pentagonful code to be introduced on p.221. Further simple ideas for making codes that can correct multiple errors from codes that can correct only one error are discussed in section 13.7. Solution to exercise 1.12 (p.16). The probability of error of R23 is, to leading order, ) ! 3 [pb(R3)]2 = 3(3f2)2 + · · · = 27f4 + · · · , (1.59) pb(R23 whereas the probability of error of R9 is dominated by the probability of five flips, ' 9 5 ( f5(1 − f)4 ! 126f5 + · · · . (1.60) pb(R9) ! The R23 decoding procedure is therefore suboptimal, since there are noise vec-tors of weight four that cause it to make a decoding error. It has the advantage, however, of requiring smaller computational re-sources: only memorization of three bits, and counting up to three, rather than counting up to nine. This simple code illustrates an important concept. Concatenated codes are widely used in practice because concatenation allows large codes to be implemented using simple encoding and decoding hardware. Some of the best known practical codes are concatenated codes.</text>
   </page>
   <page pageID = "item4" urlID = "David Mackay Book1_5.swf" pageNumber = "5">
      <text>Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981 You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links. 2 Probability, Entropy, and Inference This chapter, and its sibling, Chapter 8, devote some time to notation. Just as the White Knight distinguished between the song, the name of the song, and what the name of the song was called (Carroll, 1998), we will sometimes need to be careful to distinguish between a random variable, the value of the random variable, and the proposition that asserts that the random variable has a particular value. In any particular chapter, however, I will use the most simple and friendly notation possible, at the risk of upsetting pure-minded readers. For example, if something is ‘true with probability 1’, I will usually simply say that it is ‘true’. i ai pi 1 a 0.0575 2 b 0.0128 3 c 0.0263 4 d 0.0285 5 e 0.0913 6 f 0.0173 7 g 0.0133 8 h 0.0313 9 i 0.0599 10 j 0.0006 11 k 0.0084 12 l 0.0335 13 m 0.0235 14 n 0.0596 15 o 0.0689 16 p 0.0192 17 q 0.0008 18 r 0.0508 19 s 0.0567 20 t 0.0706 21 u 0.0334 22 v 0.0069 23 w 0.0119 24 x 0.0073 25 y 0.0164 26 z 0.0007 27 – 0.1928 a bc d e fg 2.1 Probabilities and ensembles An ensemble X is a triple (x,AX,PX), where the outcome x is the value h ij of a random variable, which takes on one of a set of possible values, AX = {a1, a2, . . . , ai, . . . , aI}, having probabilities PX = {p1, p2, . . . , pI}, with P(x=ai) = pi, pi % 0 and k l m n + ai"AX P(x=ai) = 1. The name A is mnemonic for ‘alphabet’. One example of an ensemble is a op letter that is randomly selected from an English document. This ensemble is shown in figure 2.1. There are twenty-seven possible letters: a–z, and a space character ‘-’. q r s t u v w Abbreviations. Briefer notation will sometimes be used. For example, P(x=ai) may be written as P(ai) or P(x). Probability of a subset. If T is a subset of AX then: xy z– , P(T) = P(x&amp;T) = P(x=ai). (2.1) ai"T For example, if we define V to be vowels from figure 2.1, V = {a, e, i, o, u}, then Figure 2.1. Probability distribution over the 27 outcomes for a randomly selected letter in an English language document (estimated from The Frequently Asked Questions Manual for Linux ). The picture shows the probabilities by the areas of white squares. P(V ) = 0.06 + 0.09 + 0.06 + 0.07 + 0.03 = 0.31. (2.2) A joint ensemble XY is an ensemble in which each outcome is an ordered pair x, y with x &amp; AX = {a1, . . . , aI} and y &amp; AY = {b1, . . . , bJ}. We call P(x, y) the joint probability of x and y. Commas are optional when writing ordered pairs, so xy ' x, y. N.B. In a joint ensemble XY the two variables are not necessarily inde-pendent. 22</text>
   </page>
   <page pageID = "item5" urlID = "David Mackay Book1_6.swf" pageNumber = "6">
      <text>Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981 You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links. 2.1: Probabilities and ensembles 23 x Figure 2.2. The probability distribution over the 27×27 possible bigrams xy in an English language document, The Frequently Asked Questions Manual for Linux. abcdefghijklmnopqrstuvwxyz – a b c d e f g h i j k l m n o p q r s t u v w x y z – y Marginal probability. We can obtain the marginal probability P(x) from the joint probability P(x, y) by summation: , P(x=ai) ( P(x=ai, y). (2.3) y"AY Similarly, using briefer notation, the marginal probability of y is: , P(y) ( P(x, y). (2.4) x"AX Conditional probability P(x=ai, y=bj) if P(y=bj) )= 0. (2.5) P(x=ai | y=bj) ( P(y=bj) [If P(y=bj) = 0 then P(x=ai | y=bj) is undefined.] We pronounce P(x=ai | y=bj) ‘the probability that x equals ai, given y equals bj ’. Example 2.1. An example of a joint ensemble is the ordered pair XY consisting of two successive letters in an English document. The possible outcomes are ordered pairs such as aa, ab, ac, and zz; of these, we might expect ab and ac to be more probable than aa and zz. An estimate of the joint probability distribution for two neighbouring characters is shown graphically in figure 2.2. This joint ensemble has the special property that its two marginal dis-tributions, P(x) and P(y), are identical. They are both equal to the monogram distribution shown in figure 2.1. From this joint ensemble P(x, y) we can obtain conditional distributions, P(y | x) and P(x | y), by normalizing the rows and columns, respectively (figure 2.3). The probability P(y | x=q) is the probability distribution of the second letter given that the first letter is a q. As you can see in figure 2.3a, the two most probable values for the second letter y given</text>
   </page>
   <page pageID = "item6" urlID = "David Mackay Book1_7.swf" pageNumber = "7">
      <text>Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981 You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links. 24 2 — Probability, Entropy, and Inference x x Figure 2.3. Conditional probability distributions. (a) P(y | x): Each row shows the conditional distribution of the second letter, y, given the first letter, x, in a bigram xy. (b) P(x | y): Each column shows the conditional distribution of the first letter, x, given the second letter, y. abcdefghijklmnopqrstuvwxyz abcdefghijklmnopqrstuvwxyz –– ab c d e f g h i j k lm n o p q r s t u vw x y z – y a b c d ef g h i j k l m n op q r s t u v w x y z – y (a) P(y | x) (b) P(x | y) that the first letter x is q are u and -. (The space is common after q because the source document makes heavy use of the word FAQ.) The probability P(x | y=u) is the probability distribution of the first letter x given that the second letter y is a u. As you can see in figure 2.3b the two most probable values for x given y=u are n and o. Rather than writing down the joint probability directly, we often define an ensemble in terms of a collection of conditional probabilities. The following rules of probability theory will be useful. (H denotes assumptions on which the probabilities are based.) Product rule – obtained from the definition of conditional probability: P(x, y |H) = P(x | y,H)P(y |H) = P(y | x,H)P(x |H). (2.6) This rule is also known as the chain rule. Sum rule – a rewriting of the marginal probability definition: , P(x |H) = P(x, y |H) (2.7) y , = P(x | y,H)P(y |H). (2.8) y Bayes’ theorem – obtained from the product rule: P(y | x,H) = P(x | y,H)P(y |H) (2.9) P(x |H) = +P(x | y,H)P(y |H) y! P(x | y!,H)P(y! |H) . (2.10) Independence. Two random variables X and Y are independent (sometimes written X*Y ) if and only if P(x, y) = P(x)P(y). (2.11) Exercise 2.2.[1, p.40] Are the random variables X and Y in the joint ensemble of figure 2.2 independent?</text>
   </page>
   <page pageID = "item7" urlID = "David Mackay Book1_8.swf" pageNumber = "8">
      <text>Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981 You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links. 2.2: The meaning of probability 25 I said that we often define an ensemble in terms of a collection of condi-tional probabilities. The following example illustrates this idea. Example 2.3. Jo has a test for a nasty disease. We denote Jo’s state of health by the variable a and the test result by b. a = 1 Jo has the disease a = 0 Jo does not have the disease. (2.12) The result of the test is either ‘positive’ (b = 1) or ‘negative’ (b = 0); the test is 95% reliable: in 95% of cases of people who really have the disease, a positive result is returned, and in 95% of cases of people who do not have the disease, a negative result is obtained. The final piece of background information is that 1% of people of Jo’s age and background have the disease. OK – Jo has the test, and the result is positive. What is the probability that Jo has the disease? Solution. We write down all the provided probabilities. The test reliability specifies the conditional probability of b given a: P(b=1 | a=1) = 0.95 P(b=1 | a=0) = 0.05 P(b=0 | a=1) = 0.05 P(b=0 | a=0) = 0.95; (2.13) and the disease prevalence tells us about the marginal probability of a: P(a=1) = 0.01 P(a=0) = 0.99. (2.14) From the marginal P(a) and the conditional probability P(b | a) we can deduce the joint probability P(a, b) = P(a)P(b | a) and any other probabilities we are interested in. For example, by the sum rule, the marginal probability of b=1 – the probability of getting a positive result – is P(b=1) = P(b=1 | a=1)P(a=1) + P(b=1 | a=0)P(a=0). (2.15) Jo has received a positive result b=1 and is interested in how plausible it is that she has the disease (i.e., that a=1). The man in the street might be duped by the statement ‘the test is 95% reliable, so Jo’s positive result implies that there is a 95% chance that Jo has the disease’, but this is incorrect. The correct solution to an inference problem is found using Bayes’ theorem. P(a=1 | b=1) = P(b=1 | a=1)P(a=1) (2.16) P(b=1 | a=1)P(a=1) + P(b=1 | a=0)P(a=0) 0.95 × 0.01 = (2.17) 0.95 × 0.01 + 0.05 × 0.99 = 0.16. (2.18) So in spite of the positive result, the probability that Jo has the disease is only 16%. ! 2.2 The meaning of probability Probabilities can be used in two ways. Probabilities can describe frequencies of outcomes in random experiments, but giving noncircular definitions of the terms ‘frequency’ and ‘random’ is a challenge – what does it mean to say that the frequency of a tossed coin’s</text>
   </page>
   <page pageID = "item8" urlID = "David Mackay Book1_9.swf" pageNumber = "9">
      <text>Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981 You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links. 26 2 — Probability, Entropy, and Inference Box 2.4. The Cox axioms. If a set of beliefs satisfy these axioms then they can be mapped onto probabilities satisfying P(false) = 0, P(true) = 1, 0 " P(x) " 1, and the rules of probability: Notation. Let ‘the degree of belief in proposition x’ be denoted by B(x). The negation of x (not-x) is written x. The degree of belief in a condi-tional proposition, ‘x, assuming proposition y to be true’, is represented by B(x | y). Axiom 1. Degrees of belief can be ordered; if B(x) is ‘greater’ than B(y), and B(y) is ‘greater’ than B(z), then B(x) is ‘greater’ than B(z). [Consequence: beliefs can be mapped onto real numbers.] P(x) = 1 − P(x), and P(x, y) = P(x | y)P(y). Axiom 2. The degree of belief in a proposition x and its negation x are related. There is a function f such that B(x) = f[B(x)]. Axiom 3. The degree of belief in a conjunction of propositions x, y (x and y) is related to the degree of belief in the conditional proposition x | y and the degree of belief in the proposition y. There is a function g such that B(x, y) = g [B(x | y),B(y)] . coming up heads is 1/2? If we say that this frequency is the average fraction of heads in long sequences, we have to define ‘average’; and it is hard to define ‘average’ without using a word synonymous to probability! I will not attempt to cut this philosophical knot. Probabilities can also be used, more generally, to describe degrees of be-lief in propositions that do not involve random variables – for example ‘the probability that Mr. S. was the murderer of Mrs. S., given the evidence’ (he either was or wasn’t, and it’s the jury’s job to assess how probable it is that he was); ‘the probability that Thomas Jefferson had a child by one of his slaves’; ‘the probability that Shakespeare’s plays were written by Francis Bacon’; or, to pick a modern-day example, ‘the probability that a particular signature on a particular cheque is genuine’. The man in the street is happy to use probabilities in both these ways, but some books on probability restrict probabilities to refer only to frequencies of outcomes in repeatable random experiments. Nevertheless, degrees of belief can be mapped onto probabilities if they sat-isfy simple consistency rules known as the Cox axioms (Cox, 1946) (figure 2.4). Thus probabilities can be used to describe assumptions, and to describe in-ferences given those assumptions. The rules of probability ensure that if two people make the same assumptions and receive the same data then they will draw identical conclusions. This more general use of probability to quantify beliefs is known as the Bayesian viewpoint. It is also known as the subjective interpretation of probability, since the probabilities depend on assumptions. Advocates of a Bayesian approach to data modelling and pattern recognition do not view this subjectivity as a defect, since in their view, you cannot do inference without making assumptions. In this book it will from time to time be taken for granted that a Bayesian approach makes sense, but the reader is warned that this is not yet a globally held view – the field of statistics was dominated for most of the 20th century by non-Bayesian methods in which probabilities are allowed to describe only random variables. The big difference between the two approaches is that</text>
   </page>
   <page pageID = "item9" urlID = "David Mackay Book1_10.swf" pageNumber = "10">
      <text>Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981 You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links. 2.3: Forward probabilities and inverse probabilities 27 Bayesians also use probabilities to describe inferences. 2.3 Forward probabilities and inverse probabilities Probability calculations often fall into one of two categories: forward prob-ability and inverse probability. Here is an example of a forward probability problem: Exercise 2.4.[2, p.40] An urn contains K balls, of which B are black and W = K−B are white. Fred draws a ball at random from the urn and replaces it, N times. (a) What is the probability distribution of the number of times a black ball is drawn, nB? (b) What is the expectation of nB? What is the variance of nB? What is the standard deviation of nB? Give numerical answers for the cases N = 5 and N = 400, when B = 2 and K = 10. Forward probability problems involve a generative model that describes a pro-cess that is assumed to give rise to some data; the task is to compute the probability distribution or expectation of some quantity that depends on the data. Here is another example of a forward probability problem: Exercise 2.5.[2, p.40] An urn contains K balls, of which B are black and W = K − B are white. We define the fraction fB ( B/K. Fred draws N times from the urn, exactly as in exercise 2.4, obtaining nB blacks, and computes the quantity (nB − fBN)2 NfB(1 − fB) . (2.19) z = What is the expectation of z? In the case N = 5 and fB = 1/5, what is the probability distribution of z? What is the probability that z &lt; 1? [Hint: compare z with the quantities computed in the previous exercise.] Like forward probability problems, inverse probability problems involve a generative model of a process, but instead of computing the probability distri-bution of some quantity produced by the process, we compute the conditional probability of one or more of the unobserved variables in the process, given the observed variables. This invariably requires the use of Bayes’ theorem. Example 2.6. There are eleven urns labelled by u &amp; {0, 1, 2, . . . , 10}, each con-taining ten balls. Urn u contains u black balls and 10 − u white balls. Fred selects an urn u at random and draws N times with replacement from that urn, obtaining nB blacks and N − nB whites. Fred’s friend, Bill, looks on. If after N = 10 draws nB = 3 blacks have been drawn, what is the probability that the urn Fred is using is urn u, from Bill’s point of view? (Bill doesn’t know the value of u.) Solution. The joint probability distribution of the random variables u and nB can be written P(u, nB |N) = P(nB | u,N)P(u). (2.20) From the joint probability of u and nB, we can obtain the conditional distribution of u given nB: P(u | nB,N) = P(u, nB |N) (2.21) P(nB |N) = P(nB | u,N)P(u) P(nB |N) . (2.22)</text>
   </page>
   <page pageID = "item11" urlID = "David Mackay Book1_11.swf" pageNumber = "11">
      <text>Copyright Cambridge University Press 2003. On-screen viewing permitted. Printing not permitted. http://www.cambridge.org/0521642981 You can buy this book for 30 pounds or $50. See http://www.inference.phy.cam.ac.uk/mackay/itila/ for links. 28 2 — Probability, Entropy, and Inference u Figure 2.5. Joint probability of u and nB for Bill and Fred’s urn problem, after N = 10 draws. 0123456789 10 0 1 2 3 4 5 6 7 8 9 10 nB The marginal probability of u is P(u) = 1 11 for all u. You wrote down the probability of nB given u and N, P(nB | u,N), when you solved exercise 2.4 (p.27). [You are doing the highly recommended exercises, aren’t you?] If we define fu ( u/10 then 0.3 0.25 0.2 0.15 ' N nB ( fnB 0.1 0.05 u (1 − fu)N−nB. (2.23) P(nB | u,N) = 0 0 1 2 3 4 5 6 7 8 9 10 u What about the denominator, P(nB |N)? This is the marginal probability of nB, which we can obtain using the sum rule: u P(u | nB =3,N) 0 0 1 0.063 2 0.22 3 0.29 4 0.24 5 0.13 6 0.047 7 0.0099 8 0.00086 9 0.0000096 10 0 , , P(nB |N) = P(u, nB |N) = P(u)P(nB | u,N). (2.24) u u So the conditional probability of u given nB is P(u | nB,N) = P(u)P(nB | u,N) (2.25) P(nB |N) ' N nB ( fnB 1 1 11 = u (1 − fu)N−nB. (2.26) P(nB |N) This conditional distribution can be found by normalizing column 3 of figure 2.5 and is shown in figure 2.6. The normalizing constant, the marginal probability of nB, is P(nB =3 |N =10) = 0.083. The posterior probability (2.26) is correct for all u, including the end-points u=0 and u=10, where fu = 0 and fu = 1 respectively. The posterior probability that u=0 given nB =3 is equal to zero, because if Fred were drawing from urn 0 it would be impossible for any black balls to be drawn. The posterior probability that u=10 is also zero, because there are no white balls in that urn. The other hypotheses u=1, u=2, . . . u=9 all have non-zero posterior probability. ! Figure 2.6. Conditional probability of u given nB =3 and N =10. Terminology of inverse probability In inverse probability problems it is convenient to give names to the proba-bilities appearing in Bayes’ theorem. In equation (2.25), we call the marginal probability P(u) the prior probability of u, and P(nB | u,N) is called the like-lihood of u. It is important to note that the terms likelihood and probability are not synonyms. The quantity P(nB | u,N) is a function of both nB and u. For fixed u, P(nB | u,N) defines a probability over nB. For fixed nB, P(nB | u,N) defines the likelihood of u.</text>
   </page>
</FlipBook>